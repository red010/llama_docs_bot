{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LlamaIndex Bottoms-Up Development - LLMs and Prompts\n",
    "This notebook walks through testing an LLM using the primary prompt templates used in llama-index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from langchain.llms import OpenAI\n",
    "from IPython.display import display, HTML\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "In this section, we load a test document, create an LLM, and copy prompts from llama-index to test with."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's load a quick document to test with. Right now, we will just load it as plain text, but we can do other operations later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../docs/getting_started/starter_example.md\", \"r\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create our LLM!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "llm = OpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "# llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0) # Old model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LlamaIndex uses some simple templates under the hood for answering queries -- mainly a `text_qa_template` for obtaining initial answers, and a `refine_template` for refining an existing answer when all the text does not fit into one LLM call.\n",
    "\n",
    "Let's copy the default templates, and test out our LLM with a few questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_index import Prompt\n",
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "text_qa_template = PromptTemplate(\n",
    "    \"Context information is below.\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Given the context information and not prior knowledge, \"\n",
    "    \"answer the question: {query_str}\\n\"\n",
    ")\n",
    "\n",
    "refine_template = PromptTemplate(\n",
    "    \"We have the opportunity to refine the original answer \"\n",
    "    \"(only if needed) with some more context below.\\n\"\n",
    "    \"------------\\n\"\n",
    "    \"{context_msg}\\n\"\n",
    "    \"------------\\n\"\n",
    "    \"Given the new context, refine the original answer to better \"\n",
    "    \"answer the question: {query_str}. \"\n",
    "    \"If the context isn't useful, output the original answer again.\\n\"\n",
    "    \"Original Answer: {existing_answer}\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets test a few questions!\n",
    "\n",
    "## Text QA Template Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To install LlamaIndex, you should first follow the installation steps provided in the documentation. Although the specific installation steps are not detailed in the provided context, you can typically find them in the `installation.md` file mentioned in the tip at the beginning of the document. \n",
      "\n",
      "After completing the installation steps, you can clone the LlamaIndex repository using the following command:\n",
      "\n",
      "```bash\n",
      "$ git clone https://github.com/jerryjliu/llama_index.git\n",
      "```\n",
      "\n",
      "Then, navigate to the cloned repository:\n",
      "\n",
      "```bash\n",
      "$ cd llama_index\n",
      "```\n",
      "\n",
      "From there, you can explore the contents and proceed with using LlamaIndex as described in the tutorial.\n"
     ]
    }
   ],
   "source": [
    "question = \"How can I install llama-index?\"\n",
    "prompt = text_qa_template.format(context_str=text, query_str=question)\n",
    "response = llm.complete(prompt)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To create an index using LlamaIndex, follow these steps:\n",
      "\n",
      "1. **Load the Documents**: Use the `SimpleDirectoryReader` to load the documents from a specified directory (in this case, the `data` folder).\n",
      "\n",
      "   ```python\n",
      "   from llama_index import VectorStoreIndex, SimpleDirectoryReader\n",
      "\n",
      "   documents = SimpleDirectoryReader('data').load_data()\n",
      "   ```\n",
      "\n",
      "2. **Build the Index**: Create an index from the loaded documents using `VectorStoreIndex`.\n",
      "\n",
      "   ```python\n",
      "   index = VectorStoreIndex.from_documents(documents)\n",
      "   ```\n",
      "\n",
      "3. **Query the Index**: You can then create a query engine from the index and perform queries.\n",
      "\n",
      "   ```python\n",
      "   query_engine = index.as_query_engine()\n",
      "   response = query_engine.query(\"What did the author do growing up?\")\n",
      "   print(response)\n",
      "   ```\n",
      "\n",
      "This process builds an index over the documents in the specified folder, allowing you to query the indexed data.\n"
     ]
    }
   ],
   "source": [
    "question = \"How do I create an index?\"\n",
    "prompt = text_qa_template.format(context_str=text, query_str=question)\n",
    "response = llm.complete(prompt)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "from llama_index import VectorStoreIndex, SimpleDirectoryReader\n",
      "\n",
      "documents = SimpleDirectoryReader('data').load_data()\n",
      "index = VectorStoreIndex.from_documents(documents)\n",
      "```"
     ]
    }
   ],
   "source": [
    "question = \"How do I create an index? Write your answer using only code.\"\n",
    "prompt = text_qa_template.format(context_str=text, query_str=question)\n",
    "response_gen = llm.stream_complete(prompt)\n",
    "for response in response_gen:\n",
    "    print(response.delta, end=\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refine Template Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To create an index using LlamaIndex, follow these code steps:\n",
      "\n",
      "```python\n",
      "# Step 1: Import necessary modules\n",
      "from llama_index import VectorStoreIndex, SimpleDirectoryReader\n",
      "\n",
      "# Step 2: Load documents from the 'data' folder\n",
      "documents = SimpleDirectoryReader('data').load_data()\n",
      "\n",
      "# Step 3: Build the index\n",
      "index = VectorStoreIndex.from_documents(documents)\n",
      "\n",
      "# Step 4: Persist the index to disk\n",
      "index.storage_context.persist()\n",
      "\n",
      "# Step 5: To reload the index from disk\n",
      "from llama_index import StorageContext, load_index_from_storage\n",
      "\n",
      "# Rebuild storage context\n",
      "storage_context = StorageContext.from_defaults(persist_dir=\"./storage\")\n",
      "# Load index\n",
      "index = load_index_from_storage(storage_context)\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "question = \"How do I create an index? Write your answer using only code.\"\n",
    "existing_answer = \"\"\"To create an index using LlamaIndex, you need to follow these steps:\n",
    "\n",
    "1. Download the LlamaIndex repository by cloning it from GitHub.\n",
    "2. Navigate to the `examples/paul_graham_essay` folder in the cloned repository.\n",
    "3. Create a new Python file and import the necessary modules: `VectorStoreIndex` and `SimpleDirectoryReader`.\n",
    "4. Load the documents from the `data` folder using `SimpleDirectoryReader('data').load_data()`.\n",
    "5. Build the index using `VectorStoreIndex.from_documents(documents)`.\n",
    "6. To persist the index to disk, use `index.storage_context.persist()`.\n",
    "7. To reload the index from disk, use the `StorageContext` and `load_index_from_storage` functions.\n",
    "\n",
    "Note: This answer assumes that you have already installed LlamaIndex and have the necessary dependencies.\"\"\"\n",
    "prompt = refine_template.format(context_msg=text, query_str=question, existing_answer=existing_answer)\n",
    "response = llm.complete(prompt)\n",
    "print(response.text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat Example\n",
    "The LLM also has a `chat` method that takes in a list of messages, to simulate a chat session. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant: To create an index using LlamaIndex, you typically follow these steps:\n",
      "\n",
      "1. **Install LlamaIndex**: If you haven't already, make sure to install the LlamaIndex library. You can do this using pip:\n",
      "\n",
      "   ```bash\n",
      "   pip install llama-index\n",
      "   ```\n",
      "\n",
      "2. **Import Required Modules**: In your Python script or notebook, import the necessary modules from LlamaIndex.\n",
      "\n",
      "   ```python\n",
      "   from llama_index import Document, GPTSimpleVectorIndex\n",
      "   ```\n",
      "\n",
      "3. **Prepare Your Documents**: Create a list of documents that you want to index. Each document can be represented as a `Document` object.\n",
      "\n",
      "   ```python\n",
      "   documents = [\n",
      "       Document(\"This is the first document.\"),\n",
      "       Document(\"This is the second document.\"),\n",
      "       Document(\"This is the third document.\")\n",
      "   ]\n",
      "   ```\n",
      "\n",
      "4. **Create the Index**: Use the `GPTSimpleVectorIndex` (or another index type depending on your needs) to create an index from your documents.\n",
      "\n",
      "   ```python\n",
      "   index = GPTSimpleVectorIndex(documents)\n",
      "   ```\n",
      "\n",
      "5. **Query the Index**: Once the index is created, you can query it to retrieve relevant documents based on your input.\n",
      "\n",
      "   ```python\n",
      "   response = index.query(\"What is the first document about?\")\n",
      "   print(response)\n",
      "   ```\n",
      "\n",
      "6. **Save and Load the Index (Optional)**: If you want to save the index for later use, you can serialize it to a file and load it back when needed.\n",
      "\n",
      "   ```python\n",
      "   # Save the index\n",
      "   index.save_to_file(\"my_index.json\")\n",
      "\n",
      "   # Load the index\n",
      "   loaded_index = GPTSimpleVectorIndex.load_from_file(\"my_index.json\")\n",
      "   ```\n",
      "\n",
      "Make sure to refer to the official LlamaIndex documentation for more detailed information and advanced features.\n"
     ]
    }
   ],
   "source": [
    "# from llama_index.llms import ChatMessage\n",
    "from llama_index.core.base.llms.types import ChatMessage\n",
    "\n",
    "chat_history = [\n",
    "    ChatMessage(role=\"system\", content=\"You are a helpful QA chatbot that can answer questions about llama-index.\"),\n",
    "    ChatMessage(role=\"user\", content=\"How do I create an index?\"),\n",
    "]\n",
    "\n",
    "response = llm.chat(chat_history)\n",
    "print(response.message)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we covered the low-level LLM API, and tested out some basic prompts with out documentation data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
