{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LlamaIndex Bottoms-Up Development - Documents and Nodes\n",
    "In order to answer questions about the LlamaIndex docs, we first need to load them!\n",
    "\n",
    "A majority of our documentation is in markdown format. For the sake of scope, we will ONLY worry about markdown files for now.\n",
    "\n",
    "When parsing these files, there are a few things we might want to keep track of\n",
    "\n",
    "- Current header (and header hierarchy!)\n",
    "- Code blocks\n",
    "- Text\n",
    "- Source file names\n",
    "\n",
    "While LlamaIndex DOES HAVE a built-in markdown loader, we can write our own to fit our requirements exactly! Loaders are not magic -- they just read files and create documents. So building our own is easy!\n",
    "\n",
    "We have provided an implementation of a custom markdown loaded in the source code. Let's test it out to see how it works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.join(os.getcwd(), '..')) # Allows Python to import modules from the parent directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core.node_parser import MarkdownNodeParser\n",
    "\n",
    "def load_markdown_docs(filepath):\n",
    "    \"\"\"Load markdown docs from a directory, including only Markdown files.\"\"\"\n",
    "    markdown_parser = MarkdownNodeParser()\n",
    "    \n",
    "    try:\n",
    "        loader = SimpleDirectoryReader(\n",
    "            input_dir=filepath,\n",
    "            required_exts=[\".md\"],\n",
    "            recursive=True\n",
    "        )\n",
    "\n",
    "        documents = loader.load_data()\n",
    "        nodes = []\n",
    "        for doc in documents:\n",
    "            nodes.extend(markdown_parser.get_nodes_from_documents([doc]))\n",
    "        \n",
    "        return nodes\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading documents from {filepath}: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "# Dictionary of document categories and their corresponding paths\n",
    "doc_paths = {\n",
    "    \"getting_started\": \"../docs/getting_started\",\n",
    "    \"community\": \"../docs/community\",\n",
    "    \"data\": \"../docs/core_modules/data_modules\",\n",
    "    \"agent\": \"../docs/core_modules/agent_modules\",\n",
    "    \"model\": \"../docs/core_modules/model_modules\",\n",
    "    \"query\": \"../docs/core_modules/query_modules\",\n",
    "    \"supporting\": \"../docs/core_modules/supporting_modules\",\n",
    "    \"tutorials\": \"../docs/end_to_end_tutorials\",\n",
    "    \"contributing\": \"../docs/development\"\n",
    "}\n",
    "\n",
    "# Load documents for each category\n",
    "# Loads Markdown documents from the specified file paths and stores them in a dictionary of document collections, where the keys are the document categories and the values are lists of the loaded documents.\n",
    "# This code is used to load all the Markdown documentation files for the LlamaIndex project, organized by different categories such as \"getting_started\", \"community\", \"data\", \"agent\", \"model\", \"query\", \"supporting\", \"tutorials\", and \"contributing\". The loaded documents can then be accessed by their category keys in the resulting `doc_collections` dictionary.\n",
    "doc_collections = {category: load_markdown_docs(path) for category, path in doc_paths.items()}\n",
    "\n",
    "# You can now access the documents like this:\n",
    "# getting_started_docs = doc_collections[\"getting_started\"]\n",
    "# community_docs = doc_collections[\"community\"]\n",
    "# ... and so on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "getting_started_docs = doc_collections[\"getting_started\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make our printing look nice\n",
    "from llama_index.core.schema import MetadataMode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_path: /Users/enricobusto/Documents/SOFTWARE/LlamaIndex/llama_docs_bot/2_documents_nodes/../docs/core_modules/agent_modules/agents/root.md\n",
      "file_name: root.md\n",
      "file_size: 2340\n",
      "creation_date: 2024-07-23\n",
      "last_modified_date: 2024-07-23\n",
      "\n",
      "Reasoning Loop\n",
      "The reasoning loop depends on the type of agent. We have support for the following agents: \n",
      "- OpenAI Function agent (built on top of the OpenAI Function API)\n",
      "- a ReAct agent (which works across any chat/text completion endpoint).\n"
     ]
    }
   ],
   "source": [
    "# print(agent_docs[5].get_content(metadata_mode=MetadataMode.ALL))\n",
    "print(doc_collections[\"agent\"][5].get_content(metadata_mode=MetadataMode.ALL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'file_path': '/Users/enricobusto/Documents/SOFTWARE/LlamaIndex/llama_docs_bot/2_documents_nodes/../docs/core_modules/agent_modules/agents/root.md', 'file_name': 'root.md', 'file_size': 2340, 'creation_date': '2024-07-23', 'last_modified_date': '2024-07-23'}\n"
     ]
    }
   ],
   "source": [
    "#print(agent_docs[0].metadata)\n",
    "print(doc_collections[\"agent\"][5].metadata)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks not bad! We can see that we have metadata, as well as nicely formatted content.\n",
    "\n",
    "But, we can improve the formatting even further! We can provide better templating, so that the LLM and embedding models can get a better idea of what they are reading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_template = \"Content Metadata:\\n{metadata_str}\\n\\nContent:\\n{content}\"\n",
    "metadata_template = \"{key}: {value},\"\n",
    "metadata_separator = \" \"\n",
    "\n",
    "# Assuming doc_collections is already defined and populated\n",
    "agent_docs = doc_collections.get(\"agent\", [])\n",
    "\n",
    "for doc in agent_docs:\n",
    "    if hasattr(doc, 'set_content_template'):\n",
    "        doc.set_content_template(text_template)\n",
    "    if hasattr(doc, 'set_metadata_template'):\n",
    "        doc.set_metadata_template(metadata_template)\n",
    "    if hasattr(doc, 'set_metadata_separator'):\n",
    "        doc.set_metadata_separator(metadata_separator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_path: /Users/enricobusto/Documents/SOFTWARE/LlamaIndex/llama_docs_bot/2_documents_nodes/../docs/core_modules/agent_modules/agents/root.md\n",
      "file_name: root.md\n",
      "file_size: 2340\n",
      "creation_date: 2024-07-23\n",
      "last_modified_date: 2024-07-23\n",
      "\n",
      "Reasoning Loop\n",
      "The reasoning loop depends on the type of agent. We have support for the following agents: \n",
      "- OpenAI Function agent (built on top of the OpenAI Function API)\n",
      "- a ReAct agent (which works across any chat/text completion endpoint).\n"
     ]
    }
   ],
   "source": [
    "#print(agent_docs[0].get_content(metadata_mode=MetadataMode.ALL))\n",
    "print(doc_collections[\"agent\"][5].get_content(metadata_mode=MetadataMode.ALL))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Customization\n",
    "Going even further with metadata, we can also customize which metadata fields will be seen by both the embedding model and LLM."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Hide the File Name from the LLM\n",
    "agent_docs[0].excluded_llm_metadata_keys = [\"File Name\"]\n",
    "print(agent_docs[0].get_content(metadata_mode=MetadataMode.LLM))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Hide the File Name from the embedding model\n",
    "agent_docs[0].excluded_embed_metadata_keys = [\"File Name\"]\n",
    "print(agent_docs[0].get_content(metadata_mode=MetadataMode.EMBED))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "In this notebook, we covered how to use a custom data loader, as well as how to customize the text representations of your data when including metadata for both LLMs and embedding models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
